{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba3c13c3-6e97-4450-9541-ff2133fbccf2",
   "metadata": {},
   "source": [
    "<h2>TF-IDF Implementation</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0114779f-3317-4140-be73-35328f2cd8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79e1b2c6-364f-4e5b-a390-96b03ceae3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"The best way to learn data science is by practicing.\",\n",
    "    \"TF-IDF is a statistical measure used in NLP.\",\n",
    "    \"Machine learning algorithms are essential for predictive modeling.\",\n",
    "    \"Deep learning is a subset of machine learning.\",\n",
    "    \"Vectorization converts text into numerical features.\",\n",
    "    \"The corpus should contain diverse vocabulary.\",\n",
    "    \"Statistical analysis is crucial for data science projects.\",\n",
    "    \"Natural language processing deals with text data.\",\n",
    "    \"Another common vectorizer is the CountVectorizer.\",\n",
    "    \"This example shows a small text corpus.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "457c6678-ec06-471d-b1b7-5864e77dacff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 46, 'best': 4, 'way': 53, 'to': 48, 'learn': 25, 'data': 12, 'science': 38, 'is': 23, 'by': 5, 'practicing': 34, 'tf': 45, 'idf': 20, 'statistical': 42, 'measure': 28, 'used': 49, 'in': 21, 'nlp': 31, 'machine': 27, 'learning': 26, 'algorithms': 0, 'are': 3, 'essential': 16, 'for': 19, 'predictive': 35, 'modeling': 29, 'deep': 14, 'subset': 43, 'of': 33, 'vectorization': 50, 'converts': 8, 'text': 44, 'into': 22, 'numerical': 32, 'features': 18, 'corpus': 9, 'should': 39, 'contain': 7, 'diverse': 15, 'vocabulary': 52, 'analysis': 1, 'crucial': 11, 'projects': 37, 'natural': 30, 'language': 24, 'processing': 36, 'deals': 13, 'with': 54, 'another': 2, 'common': 6, 'vectorizer': 51, 'countvectorizer': 10, 'this': 47, 'example': 17, 'shows': 40, 'small': 41}\n"
     ]
    }
   ],
   "source": [
    "v = TfidfVectorizer()\n",
    "transformed_output = v.fit_transform(corpus)\n",
    "print(v.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fcea45a0-a2c8-4673-a2d3-e9eb6381b829",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features_names = v.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "70a5588c-ba1c-4309-983a-83bb23db52aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "score = []\n",
    "for word in all_features_names:\n",
    "    index = v.vocabulary_.get(word)\n",
    "    words.append(word)\n",
    "    score.append(v.idf_[index])\n",
    "words_score = pd.DataFrame({\n",
    "    \"Words\":words,\n",
    "    \"IDF Score\":score\n",
    "})    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "81d4d5ae-238f-41ae-96b8-0424e46ed399",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Words</th>\n",
       "      <th>IDF Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>algorithms</td>\n",
       "      <td>2.704748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>analysis</td>\n",
       "      <td>2.704748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>another</td>\n",
       "      <td>2.704748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>are</td>\n",
       "      <td>2.704748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>best</td>\n",
       "      <td>2.704748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>by</td>\n",
       "      <td>2.704748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>common</td>\n",
       "      <td>2.704748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>contain</td>\n",
       "      <td>2.704748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>converts</td>\n",
       "      <td>2.704748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>corpus</td>\n",
       "      <td>2.299283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>countvectorizer</td>\n",
       "      <td>2.704748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>crucial</td>\n",
       "      <td>2.704748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>data</td>\n",
       "      <td>2.011601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>deals</td>\n",
       "      <td>2.704748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>deep</td>\n",
       "      <td>2.704748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>diverse</td>\n",
       "      <td>2.704748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>essential</td>\n",
       "      <td>2.704748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>example</td>\n",
       "      <td>2.704748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>features</td>\n",
       "      <td>2.704748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>for</td>\n",
       "      <td>2.299283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>idf</td>\n",
       "      <td>2.704748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>in</td>\n",
       "      <td>2.704748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>into</td>\n",
       "      <td>2.704748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>is</td>\n",
       "      <td>1.606136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>language</td>\n",
       "      <td>2.704748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>learn</td>\n",
       "      <td>2.704748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>learning</td>\n",
       "      <td>2.299283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>machine</td>\n",
       "      <td>2.299283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>measure</td>\n",
       "      <td>2.704748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>modeling</td>\n",
       "      <td>2.704748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>natural</td>\n",
       "      <td>2.704748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>nlp</td>\n",
       "      <td>2.704748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>numerical</td>\n",
       "      <td>2.704748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>of</td>\n",
       "      <td>2.704748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>practicing</td>\n",
       "      <td>2.704748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>predictive</td>\n",
       "      <td>2.704748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>processing</td>\n",
       "      <td>2.704748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>projects</td>\n",
       "      <td>2.704748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>science</td>\n",
       "      <td>2.299283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>should</td>\n",
       "      <td>2.704748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>shows</td>\n",
       "      <td>2.704748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>small</td>\n",
       "      <td>2.704748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>statistical</td>\n",
       "      <td>2.299283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>subset</td>\n",
       "      <td>2.704748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>text</td>\n",
       "      <td>2.011601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>tf</td>\n",
       "      <td>2.704748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>the</td>\n",
       "      <td>2.011601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>this</td>\n",
       "      <td>2.704748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>to</td>\n",
       "      <td>2.704748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>used</td>\n",
       "      <td>2.704748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>vectorization</td>\n",
       "      <td>2.704748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>vectorizer</td>\n",
       "      <td>2.704748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>vocabulary</td>\n",
       "      <td>2.704748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>way</td>\n",
       "      <td>2.704748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>with</td>\n",
       "      <td>2.704748</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Words  IDF Score\n",
       "0        algorithms   2.704748\n",
       "1          analysis   2.704748\n",
       "2           another   2.704748\n",
       "3               are   2.704748\n",
       "4              best   2.704748\n",
       "5                by   2.704748\n",
       "6            common   2.704748\n",
       "7           contain   2.704748\n",
       "8          converts   2.704748\n",
       "9            corpus   2.299283\n",
       "10  countvectorizer   2.704748\n",
       "11          crucial   2.704748\n",
       "12             data   2.011601\n",
       "13            deals   2.704748\n",
       "14             deep   2.704748\n",
       "15          diverse   2.704748\n",
       "16        essential   2.704748\n",
       "17          example   2.704748\n",
       "18         features   2.704748\n",
       "19              for   2.299283\n",
       "20              idf   2.704748\n",
       "21               in   2.704748\n",
       "22             into   2.704748\n",
       "23               is   1.606136\n",
       "24         language   2.704748\n",
       "25            learn   2.704748\n",
       "26         learning   2.299283\n",
       "27          machine   2.299283\n",
       "28          measure   2.704748\n",
       "29         modeling   2.704748\n",
       "30          natural   2.704748\n",
       "31              nlp   2.704748\n",
       "32        numerical   2.704748\n",
       "33               of   2.704748\n",
       "34       practicing   2.704748\n",
       "35       predictive   2.704748\n",
       "36       processing   2.704748\n",
       "37         projects   2.704748\n",
       "38          science   2.299283\n",
       "39           should   2.704748\n",
       "40            shows   2.704748\n",
       "41            small   2.704748\n",
       "42      statistical   2.299283\n",
       "43           subset   2.704748\n",
       "44             text   2.011601\n",
       "45               tf   2.704748\n",
       "46              the   2.011601\n",
       "47             this   2.704748\n",
       "48               to   2.704748\n",
       "49             used   2.704748\n",
       "50    vectorization   2.704748\n",
       "51       vectorizer   2.704748\n",
       "52       vocabulary   2.704748\n",
       "53              way   2.704748\n",
       "54             with   2.704748"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f75e4116-a1aa-4c36-8a2c-bc0795bb577e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The best way to learn data science is by practicing.',\n",
       " 'TF-IDF is a statistical measure used in NLP.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "121fc0d4-975f-4aae-a250-7ebe4ce99c29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.34960877,\n",
       "        0.34960877, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.26001435, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.20760498, 0.        ,\n",
       "        0.34960877, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.34960877,\n",
       "        0.        , 0.        , 0.        , 0.2971994 , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.26001435, 0.        , 0.34960877, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.34960877, 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.37594839, 0.37594839, 0.        , 0.22324599, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.37594839, 0.        ,\n",
       "        0.        , 0.37594839, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.31959048, 0.        , 0.        ,\n",
       "        0.37594839, 0.        , 0.        , 0.        , 0.37594839,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_output.toarray()[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8e53b7-2f0f-4498-93a2-4dceee427051",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
